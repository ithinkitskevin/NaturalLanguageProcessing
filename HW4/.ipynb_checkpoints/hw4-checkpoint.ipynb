{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: contractions in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
      "Requirement already satisfied: anyascii in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from gensim) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torch in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jaehw\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install contractions\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4189,
     "status": "ok",
     "timestamp": 1676582552172,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "mhxZTUmTOgif",
    "outputId": "281677d4-bf22-444b-d65e-5c9148e4f19b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jaehw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jaehw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jaehw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jaehw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jaehw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import contractions\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPR8MG3KOgii"
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96589,
     "status": "ok",
     "timestamp": 1676582648757,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "5l_0UbinOgij",
    "outputId": "8787088b-0fa9-4a0e-b43b-9d2da84935fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaehw\\AppData\\Local\\Temp\\ipykernel_19480\\4067534945.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"amazon_reviews_us_Beauty_v1_00.tsv.gz\", compression='gzip', header=0,sep='\\t', quotechar='\"', on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"amazon_reviews_us_Beauty_v1_00.tsv.gz\", compression='gzip', header=0,sep='\\t', quotechar='\"', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAVbKx34Ogik"
   },
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 2884,
     "status": "ok",
     "timestamp": 1676582651639,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "XrB_Dx5cOgil",
    "outputId": "49b52f23-ac89-48f2-b8d1-707c1fd6e7c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you Alba Bontanica!</td>\n",
       "      <td>The great thing about this cream is that it doesn't smell weird like all those chemical laden ones.  I get a nice healthy un-fake looking tan that isn't orange and it makes my skin soft too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Product, I'm 65 years old and this is all it claims to be!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>GOOD DEAL!</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps. I like that they're in bulk. It saves a lot of money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>this soaks in quick and provides a nice base for makeup</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no white cast at all and has a clean, pleasant scent. If you're a makeup wearer, this soaks in quick and provides a nice base for makeup. I've been using this brand for over a year. With daily use, this tube will last you a couple months.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094302</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Little Grooming Tool</td>\n",
       "      <td>After watching my Dad struggle with his scissors to clip, what he affectionately calls his 'tuffs of ear hair'. I bought him this electric clippers...now we do we hear him mumble..and fuss about how he is certain he will cut off his ear someday.....This is a great invention...it moves at lightenind speed and clips those hairs neaty...... Great Price too!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094303</th>\n",
       "      <td>3</td>\n",
       "      <td>Not bad for the price</td>\n",
       "      <td>Like most sound machines, the sounds choices are limited and most have a very noticeable cycle. The brook sound actually had a click at the end! However, the ocean and white noise had a good cycle.&lt;br /&gt;Unfortunately, only after a year, it broke. Now, I'm looking for a better one with more sounds.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094304</th>\n",
       "      <td>5</td>\n",
       "      <td>Best Curling Iron Ever</td>\n",
       "      <td>I bought this product because it indicated 30 second heat up time.  It is great.  You plug it in, hit the on button, select a heat level (1-15) , and in less than 30 seconds it is hot.  No more waiting around for the iron to heat up.  Quick touch ups take no time at all. I'll never go back to the &amp;quot;old style&amp;quot; plug and wait.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094305</th>\n",
       "      <td>5</td>\n",
       "      <td>The best electric toothbrush ever, REALLY!</td>\n",
       "      <td>We have used Oral-B products for 15 years; this new model is even better.  It is stronger yet thinner; generates different vibrations (3) around the toothbrush head and varies this according to pressure.  Also has a built-in timer.  Enjoy!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094306</th>\n",
       "      <td>5</td>\n",
       "      <td>Smooth and shiny teeth!</td>\n",
       "      <td>I love this toothbrush. It's easy to use, and it trains aggressive brushers (read: Type As) to treat their gums with a little more TLC. Your teeth feel cleaner longer after using a sonicare. It's almost like getting a full dental cleaning every time you brush.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5093876 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        star_rating                                          review_headline  \\\n",
       "0                 5                                               Five Stars   \n",
       "1                 5                                Thank you Alba Bontanica!   \n",
       "2                 5                                               Five Stars   \n",
       "3                 5                                               GOOD DEAL!   \n",
       "4                 5  this soaks in quick and provides a nice base for makeup   \n",
       "...             ...                                                      ...   \n",
       "5094302           5                               Great Little Grooming Tool   \n",
       "5094303           3                                    Not bad for the price   \n",
       "5094304           5                                   Best Curling Iron Ever   \n",
       "5094305           5               The best electric toothbrush ever, REALLY!   \n",
       "5094306           5                                  Smooth and shiny teeth!   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                  review_body  \n",
       "0                                                                                                                                                                                                                                                                                                                                            Love this, excellent sun block!!  \n",
       "1                                                                                                                                                                              The great thing about this cream is that it doesn't smell weird like all those chemical laden ones.  I get a nice healthy un-fake looking tan that isn't orange and it makes my skin soft too.  \n",
       "2                                                                                                                                                                                                                                                                                                            Great Product, I'm 65 years old and this is all it claims to be!  \n",
       "3                                                                                                                                                                                                                                                                        I use them as shower caps & conditioning caps. I like that they're in bulk. It saves a lot of money.  \n",
       "4                                                                                   This is my go-to daily sunblock. It leaves no white cast at all and has a clean, pleasant scent. If you're a makeup wearer, this soaks in quick and provides a nice base for makeup. I've been using this brand for over a year. With daily use, this tube will last you a couple months.  \n",
       "...                                                                                                                                                                                                                                                                                                                                                                       ...  \n",
       "5094302  After watching my Dad struggle with his scissors to clip, what he affectionately calls his 'tuffs of ear hair'. I bought him this electric clippers...now we do we hear him mumble..and fuss about how he is certain he will cut off his ear someday.....This is a great invention...it moves at lightenind speed and clips those hairs neaty...... Great Price too!  \n",
       "5094303                                                            Like most sound machines, the sounds choices are limited and most have a very noticeable cycle. The brook sound actually had a click at the end! However, the ocean and white noise had a good cycle.<br />Unfortunately, only after a year, it broke. Now, I'm looking for a better one with more sounds.  \n",
       "5094304                        I bought this product because it indicated 30 second heat up time.  It is great.  You plug it in, hit the on button, select a heat level (1-15) , and in less than 30 seconds it is hot.  No more waiting around for the iron to heat up.  Quick touch ups take no time at all. I'll never go back to the &quot;old style&quot; plug and wait.  \n",
       "5094305                                                                                                                       We have used Oral-B products for 15 years; this new model is even better.  It is stronger yet thinner; generates different vibrations (3) around the toothbrush head and varies this according to pressure.  Also has a built-in timer.  Enjoy!  \n",
       "5094306                                                                                                  I love this toothbrush. It's easy to use, and it trains aggressive brushers (read: Type As) to treat their gums with a little more TLC. Your teeth feel cleaner longer after using a sonicare. It's almost like getting a full dental cleaning every time you brush.  \n",
       "\n",
       "[5093876 rows x 3 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df = df[[\"star_rating\", \"review_headline\", \"review_body\"]]\n",
    "parsed_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1676582651639,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "AyzAB7A-o97P",
    "outputId": "68d401df-a3de-4e76-ed8e-c019df8303ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Love this, excellent sun block!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Thank you Alba Bontanica!</td>\n",
       "      <td>The great thing about this cream is that it doesn't smell weird like all those chemical laden ones.  I get a nice healthy un-fake looking tan that isn't orange and it makes my skin soft too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great Product, I'm 65 years old and this is all it claims to be!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>GOOD DEAL!</td>\n",
       "      <td>I use them as shower caps &amp; conditioning caps. I like that they're in bulk. It saves a lot of money.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>this soaks in quick and provides a nice base for makeup</td>\n",
       "      <td>This is my go-to daily sunblock. It leaves no white cast at all and has a clean, pleasant scent. If you're a makeup wearer, this soaks in quick and provides a nice base for makeup. I've been using this brand for over a year. With daily use, this tube will last you a couple months.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  star_rating                                          review_headline  \\\n",
       "0           5                                               Five Stars   \n",
       "1           5                                Thank you Alba Bontanica!   \n",
       "2           5                                               Five Stars   \n",
       "3           5                                               GOOD DEAL!   \n",
       "4           5  this soaks in quick and provides a nice base for makeup   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 review_body  \n",
       "0                                                                                                                                                                                                                                                           Love this, excellent sun block!!  \n",
       "1                                                                                             The great thing about this cream is that it doesn't smell weird like all those chemical laden ones.  I get a nice healthy un-fake looking tan that isn't orange and it makes my skin soft too.  \n",
       "2                                                                                                                                                                                                                           Great Product, I'm 65 years old and this is all it claims to be!  \n",
       "3                                                                                                                                                                                       I use them as shower caps & conditioning caps. I like that they're in bulk. It saves a lot of money.  \n",
       "4  This is my go-to daily sunblock. It leaves no white cast at all and has a clean, pleasant scent. If you're a makeup wearer, this soaks in quick and provides a nice base for makeup. I've been using this brand for over a year. With daily use, this tube will last you a couple months.  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1676582651640,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "j4nl9jctqS-G"
   },
   "outputs": [],
   "source": [
    "del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkLKJzJAOgil"
   },
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1676582655986,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "bY9eI-eHOgim"
   },
   "outputs": [],
   "source": [
    "class1_df = parsed_df.loc[parsed_df['star_rating'].isin([1,2])].sample(20000)\n",
    "class2_df = parsed_df.loc[parsed_df['star_rating'] == 3].sample(20000)\n",
    "class3_df = parsed_df.loc[parsed_df['star_rating'].isin([4,5])].sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1676582655987,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "3-lMqTr13qIT"
   },
   "outputs": [],
   "source": [
    "class1_df[\"class\"] = 1\n",
    "class2_df[\"class\"] = 2\n",
    "class3_df[\"class\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1676582656177,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "oktkperN8xsN"
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat([class1_df, class2_df, class3_df])\n",
    "\n",
    "final_df['review_headline'] = final_df['review_headline'].apply(str)\n",
    "final_df['review_body'] = final_df['review_body'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "executionInfo": {
     "elapsed": 4295,
     "status": "ok",
     "timestamp": 1676582660470,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "54Fgv27NAyCu"
   },
   "outputs": [],
   "source": [
    "final_df['review'] = final_df[['review_headline', 'review_body']].agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1676582660471,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "xYyXhK_a9B0p"
   },
   "outputs": [],
   "source": [
    "final_df = final_df.drop('star_rating', axis=1)\n",
    "final_df = final_df.drop('review_headline', axis=1)\n",
    "final_df = final_df.drop('review_body', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676582660471,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "4Sy8xVAK-uLT",
    "outputId": "33dbdcf3-8e3b-4405-d3c2-6c1601e89fb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df[final_df['class'] == 1].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1676582660711,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "Vwbcvb8BfAcm"
   },
   "outputs": [],
   "source": [
    "del(class1_df)\n",
    "del(class2_df)\n",
    "del(class3_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLX2LKJqOgim"
   },
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeJZZKpbOgim"
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676582660711,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "AKEgQuWIOgin"
   },
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "final_df[\"review\"] = final_df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1676582660712,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "dWFKHEvI_Icc"
   },
   "outputs": [],
   "source": [
    "def getRidOfNonAlphabet(s):\n",
    "  return re.sub(r\"[^a-zA-Z]+\", ' ', s)\n",
    "\n",
    "def getRidOfHTML(s):\n",
    "  return BeautifulSoup(s, \"lxml\").text\n",
    "\n",
    "def getRidOfURL(s):\n",
    "  return re.sub(r'http\\S+', '', s)\n",
    "\n",
    "def contractions(s):\n",
    "  # Also gets rid of extra spaces\n",
    "  import contractions\n",
    "  ans = []\n",
    "  for word in s.split():\n",
    "    ans.append(contractions.fix(word))\n",
    "  return ' '.join(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing by getting rid of non-alphabet characters, any HTML/URLs, and contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "executionInfo": {
     "elapsed": 28091,
     "status": "ok",
     "timestamp": 1676582688799,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "GeO8AQLccsZ6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaehw\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "final_df[\"review\"] = final_df[\"review\"].apply(getRidOfURL).apply(getRidOfHTML).apply(contractions).apply(getRidOfNonAlphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGpZkwsfOgin"
   },
   "source": [
    "## Remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1676582688800,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "qXXLxltTOgip"
   },
   "outputs": [],
   "source": [
    "def getRidOfStopWords(s):\n",
    "  stopWords = set(stopwords.words('english'))\n",
    "  words = word_tokenize(s)\n",
    "  filteredWords = []\n",
    "  for word in words:\n",
    "      if word not in stopWords:\n",
    "          filteredWords.append(word)\n",
    "  return ' '.join(filteredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "executionInfo": {
     "elapsed": 28177,
     "status": "ok",
     "timestamp": 1676582716973,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "oDE4bHHGexwm"
   },
   "outputs": [],
   "source": [
    "final_df[\"review\"] = final_df[\"review\"].apply(getRidOfStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7nOfbqQOgip"
   },
   "source": [
    "## perform lemmatization & stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1676582716974,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "aB8Ws6C5Ogiq"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "def lemmatize(s):\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words = word_tokenize(s)\n",
    "  filteredWords = []\n",
    "  for word in words:\n",
    "      filteredWords.append(lemmatizer.lemmatize(word))\n",
    "  return ' '.join(filteredWords)\n",
    "\n",
    "def stem(s):\n",
    "    filteredWords = []\n",
    "    ps = PorterStemmer()\n",
    "    for word in s.split():\n",
    "        filteredWords.append(ps.stem(word))\n",
    "    return ' '.join(filteredWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "executionInfo": {
     "elapsed": 429747,
     "status": "ok",
     "timestamp": 1676583146716,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "fBqigfruixVb"
   },
   "outputs": [],
   "source": [
    "final_df[\"formatted_review\"] = final_df[\"review\"].apply(lemmatize).apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heat hot sampl heat buy love arriv smell old like experienc previous'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(\"heat hot sampled heat buying loved arrived smelled old like experienced previously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing_df might be used to train our custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaehw\\anaconda3\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "testing_df = parsed_df\n",
    "\n",
    "testing_df['review_headline'] = testing_df['review_headline'].apply(str)\n",
    "testing_df['review_body'] = testing_df['review_body'].apply(str)\n",
    "\n",
    "testing_df['review'] = testing_df[['review_headline', 'review_body']].agg(' '.join, axis=1)\n",
    "\n",
    "testing_df[\"review\"] = testing_df[\"review\"].apply(getRidOfURL).apply(getRidOfHTML).apply(contractions).apply(getRidOfNonAlphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embedding (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vy1Md-lhHJZ2"
   },
   "source": [
    "##  a) word2vec-google-news-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 138364,
     "status": "ok",
     "timestamp": 1676584393497,
     "user": {
      "displayName": "Kevin Lee",
      "userId": "09476573521878606229"
     },
     "user_tz": 480
    },
    "id": "Yomu0IO8HH8c"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4E2dnl-4HTna"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', 0.6958590149879456)]\n"
     ]
    }
   ],
   "source": [
    "# Not my example, checking if King − Man + Woman = Queen\n",
    "predKing = wv.most_similar(positive=[\"man\",\"queen\"], negative=[\"woman\"], topn=1)\n",
    "print(predKing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TQHyT-1GMpFX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('terrific', 0.7409728765487671), ('superb', 0.7062716484069824), ('exceptional', 0.681470513343811), ('fantastic', 0.6802847385406494), ('good', 0.644292950630188), ('great', 0.6124600768089294), ('Excellent', 0.6091997623443604), ('impeccable', 0.5980966687202454), ('exemplary', 0.5959650278091431), ('marvelous', 0.5829284191131592), ('ideal', 0.5744982361793518), ('impressive', 0.5732302069664001), ('phenomenal', 0.5656530261039734), ('incredible', 0.563998281955719), ('wonderful', 0.5614796280860901), ('decent', 0.5584126114845276), ('outstanding', 0.556748628616333), ('solid', 0.5502715706825256), ('amazing', 0.5470210909843445), ('perfect', 0.540958821773529)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5567486"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Still not my example\n",
    "predOutstanding = wv.most_similar(positive=[\"excellent\"], topn=20)\n",
    "print(predOutstanding)\n",
    "\n",
    "wv.similarity(w1=\"excellent\", w2=\"outstanding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('winter', 0.6970129609107971)]\n"
     ]
    }
   ],
   "source": [
    "## Example 1 winter - cold + hot = summer\n",
    "predWinter = wv.most_similar(positive=[\"cold\",\"summer\"], negative=[\"hot\"], topn=1)\n",
    "print(predWinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italians', 0.5945354700088501), ('italian', 0.5859935283660889), ('barcelona', 0.5842419862747192), ('juve', 0.5681446194648743), ('malta', 0.564256489276886), ('spain', 0.5633350610733032), ('montreal', 0.5586921572685242), ('ac_milan', 0.5585262775421143), ('diego', 0.5546826124191284), ('real_madrid', 0.5530886054039001)]\n"
     ]
    }
   ],
   "source": [
    "## Example 2 italian - china + chinese = italy\n",
    "predAnt = wv.most_similar(positive=[\"italy\",\"chinese\"], negative=[\"china\"], topn=10)\n",
    "print(predAnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bigger', 0.7836998701095581), ('larger', 0.5866796374320984), ('Bigger', 0.5707237720489502), ('biggest', 0.5240510702133179), ('splashier', 0.5107757449150085), ('huge', 0.5084125399589539), ('sharper', 0.4824292063713074), ('heavier', 0.4784497320652008), ('tougher', 0.46322101354599), ('better', 0.4631742537021637)]\n"
     ]
    }
   ],
   "source": [
    "## Example 3 smaller - big + small = bigger\n",
    "predAnt = wv.most_similar(positive=[\"smaller\",\"big\"], negative=[\"small\"], topn=10)\n",
    "print(predAnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.01% Accuracy for Google's Word2Vec\n"
     ]
    }
   ],
   "source": [
    "score = wv.evaluate_word_analogies(\"questions-words.txt\", case_insensitive=True)[0]\n",
    "print(str(score*100) + \"% Accuracy for Google's Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vector_size': 300,\n",
    "    'window': 13,\n",
    "    'min_count': 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = [e.split() for e in final_df.review.to_list()]\n",
    "sent = [e.split() for e in testing_df.review.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sent, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not my example, checking if King − Man + Woman = Queen\n",
    "predKing = model.wv.most_similar(positive=[\"man\",\"queen\"], negative=[\"woman\"], topn=5)\n",
    "print(predKing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predKing = wv2.wv.most_similar(positive=[\"man\",\"queen\"], negative=[\"woman\"], topn=10)\n",
    "print(predKing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 1 winter - cold + hot = summer\n",
    "predWinter = wv2.wv.most_similar(positive=[\"cold\",\"summer\"], negative=[\"hot\"], topn=10)\n",
    "print(predWinter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 2 italian - china + chinese = italy\n",
    "predAnt = wv2.wv.most_similar(positive=[\"italy\",\"chinese\"], negative=[\"china\"], topn=10)\n",
    "print(predAnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example 3 smaller - big + small = bigger\n",
    "predAnt = wv.most_similar(positive=[\"smaller\",\"big\"], negative=[\"small\"], topn=10)\n",
    "print(predAnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = wv2.wv.evaluate_word_analogies(\"questions-words.txt\", case_insensitive=True)[0]\n",
    "print(str(score*100) + \"% Accuracy for My Word2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing my custom Word2Vec with Google's Word2Vec, it's clear that Google's Word2Vec is better. Although I am able to pass some examples, such as small-smaller = big-bigger like Google has, I'm not able to pass many other examples. The way I was able to pass some examples myself was by cleaning up the dataset a bit more. What I've also done was I used all of the reviews that exist in the Amazon dataset to give my Word2Vec more data, and that didn't seem to help it as much as cleaning up the dataset. In fact, increasing the size of my dataset to train my custom Word2Vec gave it an overall lower score in questions-words.txt, from 12% -> 7%. \n",
    "\n",
    "I've also utilized questions-words.txt to measure the accuracy of both Word2Vec models. This text file has load of corresponding correlated words such as \n",
    "\"Athens Greece = Baghdad Iraq\"\n",
    "all categorized by countries, food, etc. Google's Word2Vec score is higher than that of my model's as shown above, proving that Google's model encodes better rsemantic similiarites between words than my model.\n",
    "\n",
    "https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Simple models (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otmNW0ybOgiq"
   },
   "source": [
    "## TF-IDF/Word2VecFeature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Word2Vec Vectors\n",
    "def vectorizeReview(text):\n",
    "    text_vector = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            text_vector.append(wv[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if not text_vector:\n",
    "        return np.zeros(300)\n",
    "    return np.mean(text_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 Word2Vec Vectors\n",
    "def vectorizeReview10(text):\n",
    "    text_vector = []\n",
    "    i = 0\n",
    "    for ind, word in enumerate(text.split()):\n",
    "        try:\n",
    "            text_vector.append(wv2.wv[word])\n",
    "            i += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if i >= 10:\n",
    "            break\n",
    "            \n",
    "    while i < 10:\n",
    "        text_vector.append(np.zeros(300))\n",
    "        i += 1\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 20 Word2Vec Vectors\n",
    "def vectorizeReview20(text):\n",
    "    text_vector = []\n",
    "    i = 0\n",
    "    for ind, word in enumerate(text.split()):\n",
    "        try:\n",
    "            text_vector.append(wv2.wv[word])\n",
    "            i += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        if i >= 20:\n",
    "            break\n",
    "            \n",
    "    while i < 20:\n",
    "        text_vector.append(np.zeros(300))\n",
    "        i += 1\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"vector_review\"] = final_df[\"formatted_review\"].apply(vectorizeReview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"vector_review_10\"] = final_df[\"formatted_review\"].apply(vectorizeReview10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"vector_review_20\"] = final_df[\"formatted_review\"].apply(vectorizeReview10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(final_df['vector_review'], final_df['class'], test_size=0.2, random_state=42)\n",
    "train_fx, test_fx, train_fy, test_fy = train_test_split(final_df['formatted_review'], final_df['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=300)\n",
    "vectorizer.fit(final_df['formatted_review'])\n",
    "train_tfidf = vectorizer.transform(train_fx)\n",
    "test_tfidf = vectorizer.transform(test_fx)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "train_result = Encoder.fit_transform(train_y)\n",
    "test_result = Encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.21666666666667& Accuracy for SVM w/ Word2Vec\n"
     ]
    }
   ],
   "source": [
    "model = svm.LinearSVC()\n",
    "model.fit(list(train_x), train_result)\n",
    "\n",
    "result = model.predict(list(test_x))\n",
    "acc = accuracy_score(test_result, result)\n",
    "print(str(acc * 100) + \"& Accuracy for SVM w/ Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.03333333333333& Accuracy for SVM w/ TFIDF\n"
     ]
    }
   ],
   "source": [
    "model = svm.LinearSVC()\n",
    "train_tfidf = vectorizer.transform(train_fx)\n",
    "model.fit(train_tfidf, train_result)\n",
    "\n",
    "result = model.predict(test_tfidf)\n",
    "acc = accuracy_score(test_result, result)\n",
    "print(str(acc * 100) + \"& Accuracy for SVM w/ TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.31666666666667& Accuracy for Perceptron w/ Word2Vec\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(list(train_x), train_result)\n",
    "\n",
    "result = perceptron.predict(list(test_x))\n",
    "acc = accuracy_score(test_result, result)\n",
    "print(str(acc * 100) + \"& Accuracy for Perceptron w/ Word2Vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.31666666666666& Accuracy for Perceptron w/ TFIDF\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(train_tfidf, train_result)\n",
    "\n",
    "result = perceptron.predict(test_tfidf)\n",
    "acc = accuracy_score(test_result, result)\n",
    "print(str(acc * 100) + \"& Accuracy for Perceptron w/ TFIDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and Word2Vec are both NLP techinques, and initial thought I had in mind was that Word2Vec, which captures relationship between words, will give a better score than TF-IDF. But when testing the both techniques, I was unable to consistently get Word2Vec to be better than TF-IDF. Perceptron was able to get higher score commonly, but I was not able to get SVM to get higher score.\n",
    "\n",
    "My assumption in why this might be the case is that TF-IDF is better at capturing important keywords (and phrases). This might give Perceptron something to pay attention to in order to give it better class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = len(final_df)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "valid_size = 0.2\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "valid = data_utils.TensorDataset(torch.Tensor(np.vstack(final_df[\"vector_review\"])), torch.Tensor(e))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "train = data_utils.TensorDataset(torch.Tensor(np.vstack(final_df[\"vector_review\"])), torch.Tensor(e))\n",
    "train_loader = data_utils.DataLoader(train, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 33.33333333333333% Accuracy\n",
      "Epoch 1 - 33.33333333333333% Accuracy\n",
      "Epoch 2 - 43.91% Accuracy\n",
      "Epoch 3 - 45.36% Accuracy\n",
      "Epoch 4 - 45.410000000000004% Accuracy\n",
      "Epoch 5 - 46.78666666666666% Accuracy\n",
      "Epoch 6 - 47.155% Accuracy\n",
      "Epoch 7 - 47.675% Accuracy\n",
      "Epoch 8 - 63.931666666666665% Accuracy\n",
      "Epoch 9 - 64.37666666666667% Accuracy\n",
      "Best Accuracy for Perceptron model w/ avg vectors: 64.37666666666667%\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        e = torch.argmax(output, dim=1)\n",
    "        correct += (e == target).float().sum()\n",
    "\n",
    "    accuracy = correct.item()/len(valid_loader.dataset) * 100 \n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'perceptron_avg.pt')\n",
    "    print(\"Epoch \" + str(epoch) + \" - \" + str(accuracy) +\"% Accuracy\")\n",
    "print(\"Best Accuracy for Perceptron model w/ avg vectors: \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) First 10 Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "valid = data_utils.TensorDataset(torch.Tensor(final_df[\"vector_review_10\"]), torch.Tensor(e))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "train = data_utils.TensorDataset(torch.Tensor(final_df[\"vector_review_10\"]), torch.Tensor(e))\n",
    "train_loader = data_utils.DataLoader(train, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptronL, self).__init__()\n",
    "        self.fc1 = nn.Linear(300*10, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 300*10) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 54.50833333333334% Accuracy\n",
      "Epoch 1 - 56.3% Accuracy\n",
      "Epoch 2 - 56.99666666666666% Accuracy\n",
      "Epoch 3 - 57.815000000000005% Accuracy\n",
      "Epoch 4 - 58.596666666666664% Accuracy\n",
      "Epoch 5 - 59.72% Accuracy\n",
      "Epoch 6 - 60.77666666666667% Accuracy\n",
      "Epoch 7 - 62.64999999999999% Accuracy\n",
      "Epoch 8 - 64.225% Accuracy\n",
      "Epoch 9 - 65.315% Accuracy\n",
      "Best Accuracy for Perceptron model w/ first 10 words: 65.315%\n"
     ]
    }
   ],
   "source": [
    "model = PerceptronL()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        e = torch.argmax(output, dim=1)\n",
    "        correct += (e == target).float().sum()\n",
    "\n",
    "    accuracy = correct.item()/len(valid_loader.dataset) * 100 \n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'perceptron_list.pt')\n",
    "    print(\"Epoch \" + str(epoch) + \" - \" + str(accuracy) +\"% Accuracy\")\n",
    "print(\"Best Accuracy for Perceptron model w/ first 10 words: \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron model that's using the first 10 Word2Vec vectors is much better than Perceptron model using average vectors Word2Vec. This is what I thought would happen, as using average vectors will not capture any time-series aspect of the data. The average vector combines all the time-series data into a single vector, while the first 10 vectors still maintain the time-series that RNN will utilize to give better prediction. The accuracy for the first 10 vectors were sometimes 4% higher than that of average vector.\n",
    "\n",
    "The only downside I see from using first 10 vectors compared to average vectors is that average vectors utilize the whole text, even when the text length is bigger than 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recurrent Neural Networks (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "valid = data_utils.TensorDataset(torch.Tensor(final_df[\"vector_review_20\"]), torch.Tensor(e))\n",
    "valid_loader = data_utils.DataLoader(valid, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = np.array(final_df[\"class\"].astype('long')) - 1 \n",
    "train = data_utils.TensorDataset(torch.Tensor(final_df[\"vector_review_20\"]), torch.Tensor(e))\n",
    "train_loader = data_utils.DataLoader(train, batch_size = 20, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 33.33333333333333% Accuracy\n",
      "Epoch 1 - 64.43166666666667% Accuracy\n",
      "Epoch 2 - 65.94333333333333% Accuracy\n",
      "Epoch 3 - 67.08166666666666% Accuracy\n",
      "Epoch 4 - 67.71333333333334% Accuracy\n",
      "Epoch 5 - 67.64166666666667% Accuracy\n",
      "Epoch 6 - 67.26833333333333% Accuracy\n",
      "Epoch 7 - 64.56833333333334% Accuracy\n",
      "Epoch 8 - 68.17833333333333% Accuracy\n",
      "Epoch 9 - 68.26666666666667% Accuracy\n",
      "Best Accuracy for RNN model: 68.26666666666667%\n"
     ]
    }
   ],
   "source": [
    "model = MyRNN(300, 128, 3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        e = torch.argmax(output, dim=1)\n",
    "        correct += (e == target).float().sum()\n",
    "\n",
    "    accuracy = correct.item()/len(valid_loader.dataset) * 100 \n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'rnn.pt')\n",
    "    print(\"Epoch \" + str(epoch) + \" - \" + str(accuracy) +\"% Accuracy\")\n",
    "print(\"Best Accuracy for RNN model: \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        w0 = torch.zeros(self.num_layers, seq_len, self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, seq_len, self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (w0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 33.39333333333334% Accuracy\n",
      "Epoch 1 - 35.99666666666666% Accuracy\n",
      "Epoch 2 - 36.69% Accuracy\n",
      "Epoch 3 - 33.33333333333333% Accuracy\n",
      "Epoch 4 - 43.295% Accuracy\n",
      "Epoch 5 - 48.68333333333334% Accuracy\n",
      "Epoch 6 - 58.44666666666667% Accuracy\n",
      "Epoch 7 - 62.4% Accuracy\n",
      "Epoch 8 - 65.31333333333333% Accuracy\n",
      "Epoch 9 - 67.13666666666667% Accuracy\n",
      "Epoch 10 - 66.82166666666667% Accuracy\n",
      "Epoch 11 - 66.875% Accuracy\n",
      "Epoch 12 - 68.04666666666667% Accuracy\n",
      "Epoch 13 - 67.04% Accuracy\n",
      "Epoch 14 - 68.47333333333333% Accuracy\n",
      "Epoch 15 - 68.24166666666667% Accuracy\n",
      "Epoch 16 - 67.51166666666667% Accuracy\n",
      "Epoch 17 - 68.72% Accuracy\n",
      "Epoch 18 - 68.695% Accuracy\n",
      "Epoch 19 - 68.095% Accuracy\n",
      "Best Accuracy for LSTM model: 68.72%\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(300, 128, 2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        e = torch.argmax(output, dim=1)\n",
    "        correct += (e == target).float().sum()\n",
    "\n",
    "    accuracy = correct.item()/len(valid_loader.dataset) * 100 \n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'lstm.pt')\n",
    "    print(\"Epoch \" + str(epoch) + \" - \" + str(accuracy) +\"% Accuracy\")\n",
    "print(\"Best Accuracy for LSTM model: \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(MyGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        w0 = torch.zeros(self.num_layers, seq_len, self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, w0)\n",
    "\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - 41.215% Accuracy\n",
      "Epoch 1 - 58.07333333333333% Accuracy\n",
      "Epoch 2 - 63.18666666666667% Accuracy\n",
      "Epoch 3 - 65.41333333333334% Accuracy\n",
      "Epoch 4 - 66.83333333333333% Accuracy\n",
      "Epoch 5 - 67.58333333333333% Accuracy\n",
      "Epoch 6 - 66.04833333333333% Accuracy\n",
      "Epoch 7 - 66.91166666666668% Accuracy\n",
      "Epoch 8 - 68.30333333333334% Accuracy\n",
      "Epoch 9 - 66.93666666666667% Accuracy\n",
      "Epoch 10 - 68.21666666666667% Accuracy\n",
      "Epoch 11 - 68.48666666666666% Accuracy\n",
      "Epoch 12 - 68.575% Accuracy\n",
      "Epoch 13 - 68.69833333333332% Accuracy\n",
      "Epoch 14 - 68.93% Accuracy\n",
      "Epoch 15 - 66.09333333333333% Accuracy\n",
      "Epoch 16 - 68.67% Accuracy\n",
      "Epoch 17 - 69.29666666666667% Accuracy\n",
      "Epoch 18 - 69.37666666666667% Accuracy\n",
      "Epoch 19 - 69.44% Accuracy\n",
      "Best Accuracy for GRU model: 69.44%\n"
     ]
    }
   ],
   "source": [
    "model = MyGRU(300, 128, 2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "best_accuracy = float('-inf')\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        output = model(data)\n",
    "        target = target.to(dtype=torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        e = torch.argmax(output, dim=1)\n",
    "        correct += (e == target).float().sum()\n",
    "\n",
    "    accuracy = correct.item()/len(valid_loader.dataset) * 100 \n",
    "    \n",
    "    if best_accuracy < accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'gru.pt')\n",
    "    print(\"Epoch \" + str(epoch) + \" - \" + str(accuracy) +\"% Accuracy\")\n",
    "print(\"Best Accuracy for GRU model: \" + str(best_accuracy) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU > LSTM > RNN. \n",
    "\n",
    "GRU/LSTM was better than RNN (sometimes I saw >7% accuracy better) while GRU was a bit better than LSTM sometimes. RNN in this case didn't seem to face any big vanishing gradient descent problem, as our given text wasn't as big (as it was shortend to 20 vectors). But even so, LSTM & GRU consistently had higher accuracy. \n",
    "\n",
    "Not only that, but I've noticed that GRU had faster training time. This might be since LSTM requires both w0 & c0, while GRU only requires w0. The lack of c0 for GRU might have also improved the accruracy, as due to smaller dataset that we used, since GRU had fewer parameters, that meant GRU model might have avoided overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the folder will contain the Jupyter Notebook, the PDF file, and some of the saved models. Running the Jupyter Notebook will take some time (it took me 4 hours to run the whole thing), so some of the things that you can try if you want to run the notebook is...\n",
    "\n",
    "1. Utilize final_df instead of training_df. Just get rid of anything to do with training_df. This will be the in the part of pre-processing and b) Custom Word2Vec. \n",
    "2. Don't run the GRU/LSTM models, but instead use the model that I saved\n",
    "\n",
    "In the end, I was unable to get Word2Vec scores consistently higher than TF-IDF score for SVM & Perceptron. I noticed few other students had similar problem as I did, but I still think it's possible."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
