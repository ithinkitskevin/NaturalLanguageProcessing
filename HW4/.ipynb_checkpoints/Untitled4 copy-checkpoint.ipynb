{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606e6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator, Dataset, NestedField, Example\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "\n",
    "import os\n",
    "from torchtext.data import Field, NestedField, Example, Dataset, BucketIterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a9c53214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(model, iterator, TAG, num_tags):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    all_tags = []\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_amount = 0\n",
    "\n",
    "    for batch in iterator:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        words, tags, uppercase_features = batch.word, batch.tag, batch.uppercase\n",
    "        f = torch.transpose(uppercase_features, 0, 1)\n",
    "\n",
    "        predictions = model(words, f)\n",
    "\n",
    "        tags = tags.view(-1)\n",
    "        predictions = predictions.view(-1, num_tags)\n",
    "\n",
    "        labels = tags.cpu().numpy()\n",
    "        predicted_labels = torch.argmax(predictions, dim=1).cpu().numpy()\n",
    "        all_predictions.extend(predicted_labels)\n",
    "        all_tags.extend(labels)\n",
    "\n",
    "        mask = labels != 0\n",
    "        correct_predictions = (predicted_labels[mask] == labels[mask]).sum()\n",
    "        accuracy = correct_predictions / len(labels[mask])\n",
    "        \n",
    "        total_accuracy += accuracy\n",
    "        total_amount += 1\n",
    "\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(\n",
    "        all_tags,\n",
    "        all_predictions,\n",
    "        average='macro',\n",
    "        zero_division=0,\n",
    "        labels=list(range(1, len(TAG.vocab)))\n",
    "    )\n",
    "\n",
    "\n",
    "    return (total_accuracy/total_amount)*100, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b23b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = Field(sequential=False, use_vocab=False, dtype=torch.int64)\n",
    "WORD = Field(sequential=True, use_vocab=True, lower=True, batch_first=True)\n",
    "TAG = Field(sequential=True, use_vocab=True, batch_first=True)\n",
    "UPPERCASE = Field(sequential=True, use_vocab=False, dtype=torch.float, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "50c2a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentence = {\"word\": [], \"tag\": [], \"uppercase\": []}\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # End of a sentence\n",
    "                if sentence[\"word\"]:\n",
    "                    data.append(Example.fromdict(sentence, fields={\"word\": (\"word\", WORD),\n",
    "                                                                   \"tag\": (\"tag\", TAG),\n",
    "                                                                   \"uppercase\": (\"uppercase\", UPPERCASE)}))\n",
    "                    sentence = {\"index\": [], \"word\": [], \"tag\": [], \"uppercase\": []}\n",
    "                continue\n",
    "\n",
    "            index, word, tag = line.split()\n",
    "            sentence[\"word\"].append(word)\n",
    "            sentence[\"tag\"].append(tag)\n",
    "            sentence[\"uppercase\"].append(1 if word[0].isupper() else 0)\n",
    "\n",
    "    return Dataset(data, fields={\"word\": WORD, \"tag\": TAG, \"uppercase\": UPPERCASE})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bfebbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data('data/train')\n",
    "dev_data = read_data('data/dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b72f45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD.build_vocab(train_data)\n",
    "TAG.build_vocab(train_data)\n",
    "\n",
    "train_iter = BucketIterator(train_data, batch_size=1, sort_key=lambda x: len(x.word), device=torch.device('cpu'))\n",
    "dev_iter = BucketIterator(dev_data, batch_size=1, sort_key=lambda x: len(x.word), device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c954d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1ac22444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21012\n"
     ]
    }
   ],
   "source": [
    "print(len(WORD.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "252af3bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18416\\2005562340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'word'"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0].word)\n",
    "print(train_dataset[0].case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad868550",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(WORD.vocab)\n",
    "num_tags = len(TAG.vocab)\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "class BiLSTM_GLOVE(nn.Module):\n",
    "    def __init__(self, vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, embedding_matrix):\n",
    "        super(BiLSTM_GLOVE, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        self.case_embedding = nn.Embedding(2, 1)\n",
    "        self.lstm = nn.LSTM(embedding_dim + 1, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, case_x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        case_x = case_x.long()  # Convert case_x to long data type\n",
    "        case_x = self.case_embedding(case_x).squeeze(-1)\n",
    "        x = torch.cat((x, case_x.unsqueeze(-1)), dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce237cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "glove_gz_path = \"glove.6B.100d.gz\"\n",
    "glove_txt_path = \"glove.6B.100d.txt\"\n",
    "\n",
    "with gzip.open(glove_gz_path, 'rb') as f_in:\n",
    "    with open(glove_txt_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6bb5c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]])\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(glove_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e3bc571",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.zeros((len(WORD.vocab), 100))\n",
    "for i, word in enumerate(WORD.vocab.itos):\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[i] = glove_embeddings[word]\n",
    "    else:\n",
    "        # If the word is not in the GloVe vocabulary, initialize it with a random vector\n",
    "        embedding_matrix[i] = torch.rand(100) * 2 - 1  # Generate a random vector with values between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0e64aac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.56210939550724 0.08332008453380382 0.1 0.09090120675614219\n"
     ]
    }
   ],
   "source": [
    "print(accuracy,precision,recall,f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9ad4c7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - DEV accuracy: 83.3657 precision: 0.1412 recall 0.1850 f1_score 0.1373\n",
      "Epoch 2 - DEV accuracy: 87.7763 precision: 0.2464 recall 0.2840 f1_score 0.2505\n",
      "Epoch 3 - DEV accuracy: 88.8514 precision: 0.2475 recall 0.3164 f1_score 0.2734\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18416\\561833301.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jaehw\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\jaehw\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "lr\n",
    "    0.001 - leads to 76%. Good enough, but the first epoch is 76 and doesn't lead to much learning (epoch 10)\n",
    "    0.0001 - leads to good learning curve, but still only reaches to 76 after 7th epoch\n",
    "    0.00005 - horrible, still 76\n",
    "    0.002 - best, \n",
    "'''\n",
    "\n",
    "# model.embedding.weight.requires_grad = True\n",
    "\n",
    "model = BiLSTM_GLOVE(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, embedding_matrix)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        words, tags, uppercase_features = batch.word, batch.tag, batch.uppercase\n",
    "        f = torch.transpose(uppercase_features, 0, 1)\n",
    "\n",
    "        predictions = model(words, f)\n",
    "        \n",
    "        predictions = predictions.view(-1, num_tags)\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = loss_function(predictions, tags)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy,precision,recall,f1_score = validation_step(model, dev_iter, TAG, num_tags)\n",
    "    print(f\"Epoch {epoch + 1} - DEV accuracy: {accuracy:.4f} precision: {precision:.4f} recall {recall:.4f} f1_score {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0206e661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Embedding tensor([-0.6646, -0.2997, -0.3024, -0.3122, -0.5956,  0.1813, -0.5200, -0.8990,\n",
      "         0.6687, -0.0509,  0.2027, -0.3327,  0.1811,  0.9803,  0.2843, -0.0340,\n",
      "        -0.7580, -0.2475,  0.1174,  0.6668, -0.5180,  0.9893, -0.2191,  0.1035,\n",
      "        -0.5759,  0.3146,  0.7633, -0.3048,  0.3675, -0.0197,  0.2618, -0.8493,\n",
      "        -0.6716,  0.0912,  0.4773, -0.5021, -0.9139,  0.4568, -0.5791, -0.1815,\n",
      "        -0.0344,  0.1114, -0.9152,  0.7184, -0.2381, -0.7187, -0.7080, -0.2259,\n",
      "        -0.7770, -0.0739,  0.7061, -0.0501,  0.0456,  0.6191, -0.7305,  0.2588,\n",
      "         0.9421, -0.4163, -0.7916, -0.4794, -0.9400,  0.2369, -0.3231, -0.1243,\n",
      "        -0.9973, -0.5886, -0.2724,  0.6379,  0.8822, -0.0026, -0.8083, -0.2886,\n",
      "        -0.8846, -0.8714, -0.2287, -0.2878, -0.1113,  0.5348, -0.1951, -0.6304,\n",
      "         0.7549, -0.0912,  0.9251, -0.9501,  0.4688, -0.7820, -0.6845,  0.3313,\n",
      "        -0.3404, -0.6320, -0.1865,  0.6554,  0.6044, -0.7518, -0.7201,  0.7517,\n",
      "        -0.1269, -0.1848, -0.0597, -0.4081])\n",
      "OM\n",
      "Glove Forward tensor([[-0.6646, -0.2997, -0.3024, -0.3122, -0.5956,  0.1813, -0.5200, -0.8990,\n",
      "          0.6687, -0.0509,  0.2027, -0.3327,  0.1811,  0.9803,  0.2843, -0.0340,\n",
      "         -0.7580, -0.2475,  0.1174,  0.6668, -0.5180,  0.9893, -0.2191,  0.1035,\n",
      "         -0.5759,  0.3146,  0.7633, -0.3048,  0.3675, -0.0197,  0.2618, -0.8493,\n",
      "         -0.6716,  0.0912,  0.4773, -0.5021, -0.9139,  0.4568, -0.5791, -0.1815,\n",
      "         -0.0344,  0.1114, -0.9152,  0.7184, -0.2381, -0.7187, -0.7080, -0.2259,\n",
      "         -0.7770, -0.0739,  0.7061, -0.0501,  0.0456,  0.6191, -0.7305,  0.2588,\n",
      "          0.9421, -0.4163, -0.7916, -0.4794, -0.9400,  0.2369, -0.3231, -0.1243,\n",
      "         -0.9973, -0.5886, -0.2724,  0.6379,  0.8822, -0.0026, -0.8083, -0.2886,\n",
      "         -0.8846, -0.8714, -0.2287, -0.2878, -0.1113,  0.5348, -0.1951, -0.6304,\n",
      "          0.7549, -0.0912,  0.9251, -0.9501,  0.4688, -0.7820, -0.6845,  0.3313,\n",
      "         -0.3404, -0.6320, -0.1865,  0.6554,  0.6044, -0.7518, -0.7201,  0.7517,\n",
      "         -0.1269, -0.1848, -0.0597, -0.4081]], grad_fn=<EmbeddingBackward0>)\n",
      "NO GLOVE Forward tensor([[-1.1847e+00, -1.9707e+00,  7.7412e-01, -5.7076e-01,  2.7205e+00,\n",
      "         -5.2097e-01,  3.8189e-01,  1.3707e+00, -2.6614e-01,  1.4825e-01,\n",
      "         -7.6659e-01, -1.0740e+00,  2.7750e-01,  1.9377e+00, -1.3357e+00,\n",
      "          1.8414e+00,  2.7719e-01,  1.5390e-01, -2.5175e-02,  1.3527e+00,\n",
      "         -1.4749e+00, -1.7765e-01, -4.5483e-01,  7.0819e-01, -5.5615e-01,\n",
      "         -2.5219e-01, -8.1998e-01, -8.1927e-01,  5.6793e-01, -1.3443e-01,\n",
      "         -5.4135e-01,  1.7397e+00,  2.2304e-01, -7.5250e-01,  5.8620e-01,\n",
      "         -7.0972e-01, -4.2881e-01,  3.3831e-01,  1.4730e-01,  8.8106e-01,\n",
      "         -2.4035e+00, -1.3117e-01,  6.9744e-01, -8.2638e-01, -1.8969e+00,\n",
      "          1.7287e-01,  1.9316e-01,  3.5606e+00,  1.9611e-02, -2.1182e+00,\n",
      "         -6.2879e-01,  1.0150e+00,  6.0829e-01, -9.2145e-01,  1.3426e+00,\n",
      "          1.5956e+00, -3.0133e-01,  3.8667e-01, -1.8008e-03,  6.4651e-01,\n",
      "         -1.3180e+00, -1.1663e+00,  8.2414e-01,  2.0202e+00,  5.6567e-01,\n",
      "          5.6333e-02,  1.8330e-01, -1.0795e+00, -7.4475e-01,  2.4423e-01,\n",
      "          1.2786e+00, -1.1348e+00,  1.5532e+00,  4.6291e-01, -2.6084e-01,\n",
      "          1.1850e-01,  1.5453e+00,  5.4145e-01,  2.2385e-01,  1.6034e+00,\n",
      "         -3.3860e-01,  8.9838e-02, -9.3624e-01,  3.3739e-01,  1.4671e+00,\n",
      "         -8.3807e-02,  1.0329e+00, -5.6423e-01, -8.3459e-01,  4.5987e-01,\n",
      "         -1.3548e+00,  1.2279e+00,  1.0343e+00,  1.2254e+00,  8.5282e-01,\n",
      "          6.6694e-01,  8.4317e-01,  5.9110e-01, -1.4988e-01, -2.5187e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0084, -0.0427,  0.0679, -0.1092,  0.0983, -0.1093, -0.0016,  0.1013,\n",
       "          0.1938,  0.0689,  0.0850]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Original Embedding\", embedding_matrix[1968])\n",
    "f = torch.tensor([1968])\n",
    "print(WORD.vocab.itos[1968])\n",
    "model = BiLSTM_GLOVE(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, embedding_matrix)\n",
    "model(f)\n",
    "model = BiLSTM(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0cfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(21012, 100)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BiLSTM_GLOVE(vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout, embedding_matrix)\n",
    "model.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9731e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c1071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2287,  0.2687,  0.1696, -0.8279,  0.1421,  0.0879,  0.1527,  0.2460,\n",
       "        -0.6633,  0.2472,  0.7877, -0.7255, -0.2742,  0.1237,  0.4148, -0.6657,\n",
       "         0.7754, -0.3948,  0.0852,  0.5552,  0.1641,  0.0209,  0.1017, -0.2413,\n",
       "         0.0568, -0.2173, -0.6946,  0.1280, -0.4027, -0.6178, -0.8943,  0.5707,\n",
       "         0.0754, -0.1523, -0.2299,  0.2230, -0.4698,  0.2938, -0.3666, -0.4318,\n",
       "        -0.2113, -0.3330,  0.2782, -0.5088,  1.0083,  0.5549,  0.4845,  0.2277,\n",
       "         1.2120,  0.6580,  0.9587, -0.8638, -0.2186,  0.2400,  0.0465, -1.0641,\n",
       "        -0.3987, -1.5180, -0.3917, -0.3801, -0.7571, -0.1654, -1.3888,  0.1465,\n",
       "        -0.2775,  0.5438, -0.4989,  1.1819, -0.6496,  0.2811, -0.3472,  0.4645,\n",
       "         0.7467, -1.2753, -0.0139, -0.5497,  0.2577,  0.1996, -0.4776,  0.1634,\n",
       "         0.2045, -0.7177, -0.1845,  0.9144, -0.0851, -0.6853, -0.5390,  0.4724,\n",
       "         0.1316,  0.1568, -1.1030, -0.2084,  0.3321,  0.9873, -0.5288, -0.3076,\n",
       "         0.9719,  0.9091,  0.4079, -0.3955])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embeddings[WORD.vocab.itos[1968]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fedde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2287,  0.2687,  0.1696, -0.8279,  0.1421,  0.0879,  0.1527,  0.2460,\n",
       "        -0.6633,  0.2472,  0.7877, -0.7255, -0.2742,  0.1237,  0.4148, -0.6657,\n",
       "         0.7754, -0.3948,  0.0852,  0.5552,  0.1641,  0.0209,  0.1017, -0.2413,\n",
       "         0.0568, -0.2173, -0.6946,  0.1280, -0.4027, -0.6178, -0.8943,  0.5707,\n",
       "         0.0754, -0.1523, -0.2299,  0.2230, -0.4698,  0.2938, -0.3666, -0.4318,\n",
       "        -0.2113, -0.3330,  0.2782, -0.5088,  1.0083,  0.5549,  0.4845,  0.2277,\n",
       "         1.2120,  0.6580,  0.9587, -0.8638, -0.2186,  0.2400,  0.0465, -1.0641,\n",
       "        -0.3987, -1.5180, -0.3917, -0.3801, -0.7571, -0.1654, -1.3888,  0.1465,\n",
       "        -0.2775,  0.5438, -0.4989,  1.1819, -0.6496,  0.2811, -0.3472,  0.4645,\n",
       "         0.7467, -1.2753, -0.0139, -0.5497,  0.2577,  0.1996, -0.4776,  0.1634,\n",
       "         0.2045, -0.7177, -0.1845,  0.9144, -0.0851, -0.6853, -0.5390,  0.4724,\n",
       "         0.1316,  0.1568, -1.1030, -0.2084,  0.3321,  0.9873, -0.5288, -0.3076,\n",
       "         0.9719,  0.9091,  0.4079, -0.3955])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1968]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046de1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import NestedField, Dataset, Example, TabularDataset\n",
    "\n",
    "INDEX = Field(sequential=False, use_vocab=False)\n",
    "CASE = Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "WORD = Field(lower=True)\n",
    "TAG = Field()\n",
    "\n",
    "fields = [('index', INDEX), ('word', WORD), ('case', CASE), ('tag', TAG)]\n",
    "class CustomNERDataset(Dataset):\n",
    "    def __init__(self, file_path, word_field, tag_field):\n",
    "        self.word_field = word_field\n",
    "        self.tag_field = tag_field\n",
    "        self.examples = []\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            example = []\n",
    "            for line in f:\n",
    "                if not line.isspace():\n",
    "                    fields = line.strip().split()\n",
    "                    word = fields[1]\n",
    "                    tag = fields[2]\n",
    "                    example.append((word, tag))\n",
    "                else:\n",
    "                    if example:\n",
    "                        self.examples.append(example)\n",
    "                        example = []\n",
    "\n",
    "        if example:\n",
    "            self.examples.append(example)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples[index]\n",
    "        words = [self.word_field.preprocess(w) for w, _ in example]\n",
    "        tags = [self.tag_field.preprocess(t) for _, t in example]\n",
    "        return words, tags\n",
    "    \n",
    "train_dataset = CustomNERDataset('data/train', WORD, TAG)\n",
    "valid_dataset = CustomNERDataset('data/dev', WORD, TAG)\n",
    "\n",
    "WORD.build_vocab(train_dataset, min_freq=1)  # min_freq handles unknown\n",
    "TAG.build_vocab(train_dataset)\n",
    "\n",
    "class CustomBatch:\n",
    "    def __init__(self, batch):\n",
    "        self.index = batch.index\n",
    "        self.word = batch.word\n",
    "        self.case = torch.tensor([[1.0 if w[0].isupper() else 0.0 for w in words] for words in batch.word], dtype=torch.float).to(device)\n",
    "        self.tag = batch.tag\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.index, self.word, self.case, self.tag)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "train_iterator = BucketIterator(train_dataset, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.word), sort_within_batch=True, device=device, batch_class=CustomBatch)\n",
    "valid_iterator = BucketIterator(valid_dataset, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.word), sort_within_batch=True, device=device, batch_class=CustomBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05de60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor([float(val) for val in values[1:]])\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"glove.6B.100d.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
